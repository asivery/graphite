// This file was generated by lezer-generator. You probably shouldn't edit it.
import { LRParser } from "@lezer/lr";
export const parser = LRParser.deserialize({
  version: 14,
  states:
    "#SQVQPOOO[QPO'#C_OOQO'#C^'#C^QVQPOOOOQO'#Ch'#ChOaQQO,58yOOQO-E6f-E6fOOQO'#Cc'#CcOlQPO'#CbOOQO1G.e1G.eOtQPO1G.eOyQQO'#CiO!RQPO,58|OOQO7+$P7+$POOQO,59T,59TOOQO-E6g-E6g",
  stateData: "!Z~O`OS~OSPO~OTTO~OWVOXVOZXO~OYZOZUX~OZ]O~OWVOXVO~OYZOZUa~O",
  goto: "|^PP_cPPgjPPPPpvTSORTQORRYTQWTR^ZQRORURQ[WR_[",
  nodeNames:
    "⚠ Program ExprStmt Call Identifier ( Arguments Argument Number Vertex , )",
  maxTerm: 16,
  skippedNodes: [0],
  repeatNodeCount: 2,
  tokenData:
    "#n~R`X^!Tpq!Txy!xyz!}|}#S!Q![#X!c!}#a#T#o#a#y#z!T$f$g!T#BY#BZ!T$IS$I_!T$I|$JO!T$JT$JU!T$KV$KW!T&FU&FV!T~!YY`~X^!Tpq!T#y#z!T$f$g!T#BY#BZ!T$IS$I_!T$I|$JO!T$JT$JU!T$KV$KW!T&FU&FV!T~!}OT~~#SOZ~~#XOY~~#^PW~!Q![#XR#hQSPXQ!c!}#a#T#o#a",
  tokenizers: [0, 1],
  topRules: { Program: [0, 1] },
  tokenPrec: 0,
  termNames: {
    "0": "⚠",
    "1": "@top",
    "2": "ExprStmt",
    "3": "Call",
    "4": "Identifier",
    "5": '"("',
    "6": "Arguments",
    "7": "Argument",
    "8": "Number",
    "9": "Vertex",
    "10": '","',
    "11": '")"',
    "12": "statement+",
    "13": '("," Argument)+',
    "14": "␄",
    "15": "%mainskip",
    "16": "space",
  },
});
