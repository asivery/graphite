// This file was generated by lezer-generator. You probably shouldn't edit it.
import { LRParser } from "@lezer/lr";
export const parser = LRParser.deserialize({
  version: 14,
  states:
    "$UQYQPOOO_QPO'#C`OOQO'#C_'#C_QYQPOOOOQO'#Cl'#ClOdQQO,58zOOQO-E6j-E6jOrQQO'#CeOOQO'#Cd'#CdOwQPO'#CcOOQO1G.f1G.fO!PQPO1G.fO!UQPO,59PO!^QQO'#CnO!iQPO,58}OOQO7+$Q7+$QO!qQQO'#CmO!vQPO1G.kOOQO1G.k1G.kOOQO,59Y,59YOOQO-E6l-E6lOOQO,59X,59XOOQO-E6k-E6kOOQO7+$V7+$V",
  stateData:
    "#O~OeOSPOS~OTPO~OUTO~OYVOZWO^WO_YO~OZ[O~O[]O_VX~O__O~O[`O]bO~OYVOZWO^WO~O[]O_Va~OZeO~O[`O]gO~O",
  goto: "!]cPPPdhPPlouPPPPPPy!P!VTSORTQORRZTQXTRc]TWT]QRORURQa[RfaQ^XRd^",
  nodeNames:
    "⚠ Comment Program ExprStmt Call Identifier ( Arguments Argument VertexCollection [ Vertex , ] Number )",
  maxTerm: 21,
  skippedNodes: [0, 1],
  repeatNodeCount: 3,
  tokenData:
    "$r~RdX^!apq!ast#Uxy#myz#r|}#w!Q![#|!c!}$U!}#O$c#P#Q$h#R#S$m#T#o$U#y#z!a$f$g!a#BY#BZ!a$IS$I_!a$I|$JO!a$JT$JU!a$KV$KW!a&FU&FV!a~!fYe~X^!apq!a#y#z!a$f$g!a#BY#BZ!a$IS$I_!a$I|$JO!a$JT$JU!a$KV$KW!a&FU&FV!a~#ZSP~OY#UZ;'S#U;'S;=`#g<%lO#U~#jP;=`<%l#U~#rOU~~#wO_~~#|O[~~$RP^~!Q![#|R$]QTPZQ!c!}$U#T#o$U~$hOY~~$mO]~P$rOTP",
  tokenizers: [0, 1],
  topRules: { Program: [0, 2] },
  tokenPrec: 0,
  termNames: {
    "0": "⚠",
    "1": "Comment",
    "2": "@top",
    "3": "ExprStmt",
    "4": "Call",
    "5": "Identifier",
    "6": '"("',
    "7": "Arguments",
    "8": "Argument",
    "9": "VertexCollection",
    "10": '"["',
    "11": "Vertex",
    "12": '","',
    "13": '"]"',
    "14": "Number",
    "15": '")"',
    "16": "statement+",
    "17": '("," Vertex)+',
    "18": '("," Argument)+',
    "19": "␄",
    "20": "%mainskip",
    "21": "space",
  },
});
