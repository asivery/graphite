// This file was generated by lezer-generator. You probably shouldn't edit it.
import { LRParser } from "@lezer/lr";
export const parser = LRParser.deserialize({
  version: 14,
  states:
    "#SQVQPOOO[QPO'#C_OOQO'#C^'#C^QVQPOOOOQO'#Cd'#CdOaQPO,58yOOQO-E6b-E6bOOQO'#Cb'#CbOlQPO'#CaOOQO1G.e1G.eOtQPO1G.eOyQPO'#CeO!RQPO,58{OOQO7+$P7+$POOQO,59P,59POOQO-E6c-E6c",
  stateData: "!Z~O[OS~OSPO~O]TO~OSVOVVO_XO~O^ZO_TX~O_]O~OSVOVVO~O^ZO_Ta~O",
  goto: "xYPPZ_PcfPlrTSORTQORRYTQWTR^ZQRORURQ[WR_[",
  nodeNames: "⚠ Program ExprStmt Call Identifier Arguments Argument Number",
  maxTerm: 15,
  nodeProps: [["group", 2, "Statement"]],
  skippedNodes: [0],
  repeatNodeCount: 2,
  tokenData:
    "#l~R`X^!Tpq!Txy!xyz!}|}#S!Q![#X!c!}#a#T#o#a#y#z!T$f$g!T#BY#BZ!T$IS$I_!T$I|$JO!T$JT$JU!T$KV$KW!T&FU&FV!T~!YY[~X^!Tpq!T#y#z!T$f$g!T#BY#BZ!T$IS$I_!T$I|$JO!T$JT$JU!T$KV$KW!T&FU&FV!T~!}O]~~#SO_~~#XO^~~#^PV~!Q![#X~#fQS~!c!}#a#T#o#a",
  tokenizers: [0],
  topRules: { Program: [0, 1] },
  tokenPrec: 0,
  termNames: {
    "0": "⚠",
    "1": "@top",
    "2": "ExprStmt",
    "3": "Call",
    "4": "Identifier",
    "5": "Arguments",
    "6": "Argument",
    "7": "Number",
    "8": "statement+",
    "9": '("," Argument)+',
    "10": "␄",
    "11": "%mainskip",
    "12": "space",
    "13": '"("',
    "14": '","',
    "15": '")"',
  },
});
